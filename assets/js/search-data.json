{
  
    
        "post0": {
            "title": "Transformer spam classifier",
            "content": "This is a classifier which uses pretrained transformer as its embedding, implemented using allennlp 2.4.0. . Dataset: https://www.kaggle.com/team-ai/spam-text-message-classification . DatasetReader . from allennlp.data import DatasetReader, Instance from allennlp.data.fields import LabelField, TextField class ClassificationTsvReader(DatasetReader): def __init__( self, tokenizer=None, token_indexers=None, **kwargs ): super().__init__(**kwargs) self.tokenizer = tokenizer self.token_indexers = token_indexers def _read(self, file_path): with open(file_path, &quot;r&quot;) as lines: for line in lines: label, text = line.strip().split(&quot; t&quot;) tokens = self.tokenizer.tokenize(text) text_field = TextField(tokens, self.token_indexers) label_field = LabelField(label) fields = {&quot;tokens&quot;: text_field, &quot;label&quot;: label_field} yield Instance(fields) def text_to_instance(self, text: str, label: str = None) -&gt; Instance: tokens = self.tokenizer.tokenize(text) text_field = TextField(tokens, self.token_indexers) fields = {&#39;tokens&#39;: text_field} if label: fields[&#39;label&#39;] = LabelField(label) return Instance(fields) . Tokenizer . from allennlp.data.tokenizers import PretrainedTransformerTokenizer tokenizer = PretrainedTransformerTokenizer(&quot;roberta-base&quot;) . Indexer . from allennlp.data.token_indexers import PretrainedTransformerIndexer token_indexers = {&quot;tokens&quot;: PretrainedTransformerIndexer(&quot;roberta-base&quot;)} . Reading instances . train_file=&#39;spam-classifier/data/spam-train.tsv&#39; dev_file=&#39;spam-classifier/data/spam-dev.tsv&#39; dataset_reader = ClassificationTsvReader( tokenizer=tokenizer, token_indexers=token_indexers) train_instances = list(dataset_reader.read(train_file)) dev_instances = list(dataset_reader.read(dev_file)) . Vocabulary . from allennlp.data.vocabulary import Vocabulary vocab = Vocabulary() vocab = vocab.from_pretrained_transformer(model_name=&quot;roberta-base&quot;, namespace=&quot;tokens&quot;) vocab.extend_from_instances(train_instances+dev_instances) . . Token embedder . from allennlp.modules.token_embedders import PretrainedTransformerEmbedder token_embedder = PretrainedTransformerEmbedder(&quot;roberta-base&quot;) . Text-field embedder . from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder embedder = BasicTextFieldEmbedder({&quot;tokens&quot;: token_embedder}) . Encoder . from allennlp.modules.seq2vec_encoders import ClsPooler encoder = ClsPooler(embedding_dim=embedder.get_output_dim()) . Model . from allennlp.models import BasicClassifier model=BasicClassifier( vocab=vocab, text_field_embedder=embedder, seq2vec_encoder=encoder, namespace=&quot;tokens&quot;, label_namespace=&quot;labels&quot; ) . DataLoaders . from allennlp.data.data_loaders import SimpleDataLoader train_data_loader=SimpleDataLoader( instances=train_instances, batch_size=16, shuffle=True, vocab=vocab, ) dev_data_loader=SimpleDataLoader( instances=dev_instances, batch_size=16, shuffle=True, vocab=vocab ) . Trainer . from allennlp.training import GradientDescentTrainer from torch.optim import Adam trainer = GradientDescentTrainer( model=model, optimizer=Adam(model.parameters()), data_loader=train_data_loader, validation_data_loader=dev_data_loader, patience=3, num_epochs=100, ) . Preparing GPU training . import torch device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) model.to(device) train_data_loader.set_target_device(device) dev_data_loader.set_target_device(device) . Train the model . %time trainer.train() . . CPU times: user 1.98 s, sys: 105 ms, total: 2.09 s Wall time: 2.8 s . . {&#39;best_epoch&#39;: 4, &#39;best_validation_accuracy&#39;: 0.9928443649373881, &#39;best_validation_loss&#39;: 0.0313081862130535, &#39;peak_worker_0_memory_MB&#39;: 256.30078125, &#39;training_duration&#39;: &#39;0:00:02.795020&#39;, &#39;training_start_epoch&#39;: 0, &#39;training_epochs&#39;: 0, &#39;epoch&#39;: 0, &#39;training_accuracy&#39;: 1.0, &#39;training_loss&#39;: 0.001972834501566956, &#39;training_worker_0_memory_MB&#39;: 256.30078125, &#39;validation_accuracy&#39;: 0.9928443649373881, &#39;validation_loss&#39;: 0.03381697619708055} . Predictor . from allennlp.predictors.text_classifier import TextClassifierPredictor predictor = TextClassifierPredictor(model=model, dataset_reader=dataset_reader) . Test predictor . predictor.predict(&quot;hello world&quot;) . {&#39;logits&#39;: [1.1957952976226807, -1.0361652374267578], &#39;probs&#39;: [0.903083086013794, 0.09691690653562546], &#39;token_ids&#39;: [3614, 618], &#39;label&#39;: &#39;ham&#39;, &#39;tokens&#39;: [&#39;hello&#39;, &#39;world&#39;]} . Save model . model_dir = &#39;simple-classifier-spam/&#39; with open(model_dir + &quot;model.th&quot;, &#39;wb&#39;) as f: torch.save(model.state_dict(), f) vocab.save_to_files(model_dir + &quot;vocabulary&quot;) .",
            "url": "https://lenguist.github.io/site/allennlp/2021/06/04/transformer-classifer.html",
            "relUrl": "/allennlp/2021/06/04/transformer-classifer.html",
            "date": " • Jun 4, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Simple spam classifier",
            "content": "This is as simple-as-it-gets classifier, implemented using allennlp 2.4.0. . Dataset: https://www.kaggle.com/team-ai/spam-text-message-classification . DatasetReader . from allennlp.data import DatasetReader, Instance from allennlp.data.fields import LabelField, TextField class ClassificationTsvReader(DatasetReader): def __init__( self, tokenizer=None, token_indexers=None, **kwargs ): super().__init__(**kwargs) self.tokenizer = tokenizer self.token_indexers = token_indexers def _read(self, file_path): with open(file_path, &quot;r&quot;) as lines: for line in lines: label, text = line.strip().split(&quot; t&quot;) tokens = self.tokenizer.tokenize(text) text_field = TextField(tokens, self.token_indexers) label_field = LabelField(label) fields = {&quot;tokens&quot;: text_field, &quot;label&quot;: label_field} yield Instance(fields) def text_to_instance(self, text: str, label: str = None) -&gt; Instance: tokens = self.tokenizer.tokenize(text) text_field = TextField(tokens, self.token_indexers) fields = {&#39;tokens&#39;: text_field} if label: fields[&#39;label&#39;] = LabelField(label) return Instance(fields) . Tokenizer . from allennlp.data.tokenizers import WhitespaceTokenizer tokenizer = WhitespaceTokenizer() . Indexer . from allennlp.data.token_indexers import SingleIdTokenIndexer token_indexers = {&quot;tokens&quot;: SingleIdTokenIndexer()} . Reading instances . train_file=&#39;spam-classifier/data/spam-train.tsv&#39; dev_file=&#39;spam-classifier/data/spam-dev.tsv&#39; dataset_reader = ClassificationTsvReader( tokenizer=tokenizer, token_indexers=token_indexers) train_instances = list(dataset_reader.read(train_file)) dev_instances = list(dataset_reader.read(dev_file)) . Vocabulary . from allennlp.data.vocabulary import Vocabulary vocab = Vocabulary.from_instances(train_instances + dev_instances) vocab_size = vocab.get_vocab_size(&quot;tokens&quot;) . . Token embedder . from allennlp.modules.token_embedders import Embedding token_embedder=Embedding(embedding_dim=10, num_embeddings=vocab_size) . Text-field embedder . from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder embedder = BasicTextFieldEmbedder({&quot;tokens&quot;: token_embedder}) . Encoder . from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder encoder = BagOfEmbeddingsEncoder(embedding_dim=10) . Model . from allennlp.models import BasicClassifier model=BasicClassifier( vocab=vocab, text_field_embedder=embedder, seq2vec_encoder=encoder, namespace=&quot;tokens&quot;, label_namespace=&quot;labels&quot; ) . DataLoaders . from allennlp.data.data_loaders import SimpleDataLoader train_data_loader=SimpleDataLoader( instances=train_instances, batch_size=16, shuffle=True, vocab=vocab, ) dev_data_loader=SimpleDataLoader( instances=dev_instances, batch_size=16, shuffle=True, vocab=vocab ) . Trainer . from allennlp.training import GradientDescentTrainer from torch.optim import Adam trainer = GradientDescentTrainer( model=model, optimizer=Adam(model.parameters()), data_loader=train_data_loader, validation_data_loader=dev_data_loader, patience=3, num_epochs=20 ) . Preparing GPU training . import torch device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) model.to(device) train_data_loader.set_target_device(device) dev_data_loader.set_target_device(device) . Train the model . %time trainer.train() . . CPU times: user 1.98 s, sys: 105 ms, total: 2.09 s Wall time: 2.8 s . . {&#39;best_epoch&#39;: 4, &#39;best_validation_accuracy&#39;: 0.9928443649373881, &#39;best_validation_loss&#39;: 0.0313081862130535, &#39;peak_worker_0_memory_MB&#39;: 256.30078125, &#39;training_duration&#39;: &#39;0:00:02.795020&#39;, &#39;training_start_epoch&#39;: 0, &#39;training_epochs&#39;: 0, &#39;epoch&#39;: 0, &#39;training_accuracy&#39;: 1.0, &#39;training_loss&#39;: 0.001972834501566956, &#39;training_worker_0_memory_MB&#39;: 256.30078125, &#39;validation_accuracy&#39;: 0.9928443649373881, &#39;validation_loss&#39;: 0.03381697619708055} . Predictor . from allennlp.predictors.text_classifier import TextClassifierPredictor predictor = TextClassifierPredictor(model=model, dataset_reader=dataset_reader) . Test predictor . predictor.predict(&quot;hello world&quot;) . {&#39;logits&#39;: [1.1957952976226807, -1.0361652374267578], &#39;probs&#39;: [0.903083086013794, 0.09691690653562546], &#39;token_ids&#39;: [3614, 618], &#39;label&#39;: &#39;ham&#39;, &#39;tokens&#39;: [&#39;hello&#39;, &#39;world&#39;]} . Save model . model_dir = &#39;simple-classifier-spam/&#39; with open(model_dir + &quot;model.th&quot;, &#39;wb&#39;) as f: torch.save(model.state_dict(), f) vocab.save_to_files(model_dir + &quot;vocabulary&quot;) .",
            "url": "https://lenguist.github.io/site/allennlp/2021/06/04/simple-classifer.html",
            "relUrl": "/allennlp/2021/06/04/simple-classifer.html",
            "date": " • Jun 4, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Ai Book List",
            "content": "AI Book List . Last updated November 2021 . How We Learn by Stanislav Dehaene | This Is Your Brain on Music by Daniel Levitin | A Thousand Brains by Jeff Hawkins | On Intelligence by Jeff Hawkins | How to Create a Mind by Ray Kurzweil | Superintelligence by Nick Bostrom | The Language Instinct by Steven Pinker | Deep Learning for Coders by Jeremy Howard | In Search of Memory by Eric Kandel | The Emperor’s New Mind by Roger Penrose | The Feeling of Life Itself by Christof Koch | .",
            "url": "https://lenguist.github.io/site/2021/01/11/AI-book-list.html",
            "relUrl": "/2021/01/11/AI-book-list.html",
            "date": " • Jan 11, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Maksym Bondarenko, and I am an international student from Ukraine graduating from the Lawrenceville School in 2022. I am passionate about artificial intelligence and natural language processing, and I plan to major in computer science in college. I have been programming for a little over a year now, and I have found blogs and tutorials written by others to be incredibly helful in getting started with artificial intelligence and coding in general. I hope this blog can become that for someone else too! . Social media . Github:@Lenguist | Twitter:@Lenguist1 | Instagram:@maksym_bondarenko2018 | .",
          "url": "https://lenguist.github.io/site/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lenguist.github.io/site/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}